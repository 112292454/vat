# VAT 默认配置文件
# 请根据实际环境修改相关路径和参数

# =============================================================================
# 存储配置 - 所有文件和模型的路径管理
# =============================================================================
storage:
  work_dir: "/local/gzy/4090/vat/work"              # 工作目录（临时文件、处理中的文件）
  output_dir: "/local/gzy/4090/vat/data/videos"     # 输出根目录（每个视频路径 = output_dir/video_id，运行时计算）
  database_path: "/local/gzy/4090/vat/data/database.db" # 数据库路径
  models_dir: "/local/gzy/4090/vat/models"          # 所有模型文件的统一存储根目录
  resource_dir: "vat/resources"                        # 资源目录（字体、样式模板等）
  fonts_dir: "vat/resources/fonts"                     # 字体目录
  subtitle_style_dir: "vat/resources/subtitle_style"   # 字幕样式模板目录
  cache_dir: "~/.vat/cache"                            # 缓存目录
  cache_enabled: false                                   # 是否启用 diskcache（LLM/翻译结果缓存）
                                                         # 高并发时 diskcache 内部 SQLite 可能引发锁冲突，建议关闭
                                                         # 开启后重跑同一视频可复用已缓存的 LLM 调用结果

# =============================================================================
# 下载器配置
# =============================================================================
downloader:
  youtube:
    # 代理：引用全局配置 proxy.http_proxy
    format: "bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]"
    max_workers: 1                          # 同时下载数量
    
    # 字幕下载配置
    download_subtitles: true                # 是否下载字幕
    subtitle_languages: ["ja", "zh"]  # 字幕语言优先级（按顺序尝试）
                                            # 常用语言代码: ja=日语, zh=中文, en=英语, ko=韩语
    subtitle_format: "vtt"                  # 字幕格式: vtt/srt/ass
    
    # YouTube 反爬配置
    cookies_file: "cookies/www.youtube.com_cookies.txt"  # 当youtube直接下载失败时配置，参考yt-dlp官方文档导出cookie。Cookie文件路径（Netscape格式），解决 YouTube bot 检测
                                            # 留空字符串 "" 表示不使用 cookie
                                            # 获取方式: 浏览器插件导出 Netscape 格式 cookie
    remote_components: ["ejs:github"]       # yt-dlp 远程组件，解决 YouTube JS challenge
                                            # 推荐 ["ejs:github"]，留空列表 [] 表示不启用
    download_delay: 30                       # 批量下载视频间的延迟（秒），防止 YouTube 限流。0 表示不延迟
  
  # 场景识别（下载时根据标题/简介判断视频场景，极简单的分类任务）
  # 留空则继承全局 llm 配置
  scene_identify:
    model: "gpt-4o-mini"                            # 留空=使用全局 llm.model
    api_key: "${VAT_RESELLER_APIKEY}"               # 留空=使用全局 llm 配置
    base_url: "https://api.videocaptioner.cn"       # 留空=使用全局 llm 配置
  
  # 视频信息翻译（下载/同步时翻译标题、描述、标签，推荐B站分区，需要一定润色能力）
  # 留空则继承全局 llm 配置
  # video_info_translate:
  #   model: "gpt-4o-mini"                      # 若需要润色能力，可用稍好的模型
  #   api_key: "${VAT_RESELLER_APIKEY}"         # 中转站 API Key
  #   base_url: "https://api.videocaptioner.cn" # 中转站 Base URL

# =============================================================================
# 语音识别配置
# =============================================================================
asr:
  backend: "faster-whisper"                 # 后端：faster-whisper/whisper
  model: "large-v3"                         # 模型名称：tiny/base/small/medium/large/large-v2/large-v3/kotoba-tech/kotoba-whisper-v2.2
  language: "ja"                            # 默认识别语言：ja/en/zh等（留空可自动检测）
  device: "cuda"                            # 设备：cuda/cpu
  compute_type: "float32"                   # 计算精度：float16/float32/int8
  vad_filter: false                          # 语音活动检测
  beam_size: 7                              # beam搜索大小
  models_subdir: "whisper"                  # 【模型路径】子目录名称，完整路径: storage.models_dir/whisper/
                                            # 模型会自动下载到此目录
  
  # =============================================================================
  # [已废弃] Pipeline ASR 配置
  # 状态：实验性功能，已搁置
  # 原因：基于 kotoba-whisper 的多说话人识别效果不如 faster-whisper large-v3
  # 保留配置以备将来改进使用，但当前不建议启用
  # =============================================================================
  # use_pipeline: false               # 是否使用Transformers Pipeline模式（已废弃，保持 false）
  # enable_diarization: true         # 是否启用说话人分离（已废弃）
  # enable_punctuation: false         # 是否自动添加标点符号（已废弃）
  # pipeline_batch_size: 8            # Pipeline批次大小（已废弃）
  # pipeline_chunk_length: 30         # Pipeline音频分块长度（秒）（已废弃）
  # num_speakers: 1                # 固定说话人数量（已废弃）
  # min_speakers: 1                # 最小说话人数量（已废弃）
  # max_speakers: 2                # 最大说话人数量（已废弃）
  
  # 高级参数 - 提高识别质量
  word_timestamps: true                     # 启用词级时间戳（提高分割精度）
  condition_on_previous_text: false          # 基于前文的条件预测（提高连贯性）——true可能被开头的slinece识别出的幻觉字幕干扰。
  temperature: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]  # 固定温度（0.0=确定性，避免随机性导致识别不稳定）
                                            # 注：温度回退 [0.0, 0.2, ...] 会导致结果不稳定——但是定死为0又会导致幻觉，单个字符大量重复——有待探索
  compression_ratio_threshold: 2.4          # 压缩比阈值（降低可减少重复文本）
  log_prob_threshold: -1.0                  # 对数概率阈值（提高可过滤低质量片段）
  no_speech_threshold: 0.6                  # 无语音阈值（提高到0.6，保留更多边界语音）
  initial_prompt: ""                          # 初始提示词（引导模型识别特定场景，建议使用英文）
  repetition_penalty: 1.0                   # 重复惩罚（测试显示>1.0反而有害，保持默认值）
  hallucination_silence_threshold: 2        # 幻觉静默阈值（测试显示2效果略优于1和3）
  
  # VAD参数 - 仅在 vad_filter=true 时生效
  # 使用非常低的阈值和大的填充以保留全部内容
  vad_threshold: 0.02                       # VAD激活阈值（极低，避免漏检）
  vad_min_speech_duration_ms: 30            # 最小语音段时长（极短，保留短语音）
  vad_max_speech_duration_s: 9999.0         # 最大语音段时长（无限制）
  vad_min_silence_duration_ms: 20           # 最小静音时长
  vad_speech_pad_ms: 5000                  # 语音前后填充时长（5秒，确保不丢内容）
  
  # ChunkedASR 分块处理配置
  enable_chunked: true                      # 是否启用分块处理（长音频自动分块）
  chunk_length_sec: 600                     # 每块长度（秒，默认10分钟）
  chunk_overlap_sec: 10                     # 块之间重叠时长（秒）
  chunk_concurrency: 5                      # 并发处理的块数
  
  # Split 配置（嵌套）- 智能断句
  split:
    enable: true                            # 是否启用智能断句（将零碎文本重组为完整句子）
    mode: "sentence"                        # 断句模式：sentence（句子级）| semantic（语义级）
    max_words_cjk: 40                       # 中文/日文每句最大字符数（硬性限制）
    max_words_english: 24                   # 英文每句最大单词数（硬性限制）
    min_words_cjk: 6                        # 中文/日文每句最小字符数（软性建议）
    min_words_english: 1                    # 英文每句最小单词数（软性建议）
    recommend_words_cjk: 20                 # 中文/日文每句推荐字符数（软性建议，理想长度）
    recommend_words_english: 10             # 英文每句推荐单词数（软性建议，理想长度）
    model: "gpt-4o-mini"                      # 断句使用的 LLM 模型（断句对模型要求低，用便宜模型即可）
    api_key: "${VAT_RESELLER_APIKEY}"         # 中转站 API Key
    base_url: "https://api.videocaptioner.cn" # 中转站 Base URL
    
    # 模型升级链：断句失败时自动尝试更强模型（默认关闭）
    allow_model_upgrade: false              # 是否启用模型升级
    model_upgrade_chain: []                 # 模型升级顺序（从弱到强），例如: ["gemini-2.0-flash", "gemini-2.5-pro"]
    
    # 分块配置（新增）
    enable_chunking: true                   # 是否启用分块（长视频启用）
    chunk_size_sentences: 50                # 每块句子数
    chunk_overlap_sentences: 1              # 块之间重叠句子数   todo: 大于1可能有问题：LLM 在 overlap 区域内会重新断句，导致“句子条数”不稳定。
    chunk_min_threshold: 30                 # 少于此句子数不分块
  
  # 后处理配置（新增）- 幻觉检测、重复清理、日语处理
  postprocessing:
    enable_hallucination_detection: true    # 启用幻觉检测（移除 "www"、"ご視聴ありがとう" 等）
    enable_repetition_cleaning: true        # 启用重复清理（处理 "うううう" 等异常重复）
    enable_japanese_processing: true        # 启用日语特殊处理（标点标准化等）
    min_confidence: 0.8                     # 幻觉检测最小置信度
    custom_blacklist: []                    # 自定义幻觉黑名单（可添加特定需要过滤的文本）
  
  # 人声分离配置 - 使用 Mel-Band-Roformer 模型
  # 测试结论：使用 temperature=0.0 后，原始音频 ASR 已足够好
  # 人声分离对游戏直播反而有害（50s-95s 区间：27片段→11片段）
  # 建议：仅在纯音乐MV等特殊场景手动启用
  vocal_separation:
    enable: false                           # 是否启用人声分离（默认关闭）
    auto_detect_bgm: false                  # 自动检测（默认关闭，测试显示效果不佳）
    model_filename: "vocal_separator/model.ckpt"  # 模型权重路径（相对于 storage.models_dir）
                                            # 完整路径: storage.models_dir/vocal_separator/model.ckpt
                                            # 下载地址: https://github.com/KimberleyJensen/Mel-Band-Roformer-Vocal-Model
    save_accompaniment: false               # 是否保存分离出的伴奏
  
  # 注意：游戏直播、有背景音乐的视频建议关闭 vad_filter
  # 纯人声视频（采访、播客等）可以开启 vad_filter 以提高效率

# =============================================================================
# 翻译器配置
# =============================================================================
translator:
  # 翻译后端类型：llm（使用大语言模型接口）/local
  backend_type: "llm"
  
  source_language: "ja"                     # 源语言
  target_language: "zh-cn"                  # 目标语言
  skip_translate: false                     # 跳过翻译，直接使用ASR原文作为翻译结果（debug用）
  
  # LLM 翻译器配置 (在线)
  llm:
    model: "gemini-3-flash-preview"              # 翻译模型（使用全局 Google API）
    # api_key: ""                             # 留空 = 使用全局 llm 配置（Google API）
    # base_url: ""                            # 留空 = 使用全局 llm 配置（Google API）
    enable_reflect: true                    # 反思翻译（启用后质量更高，但消耗更多 token）
    batch_size: 40                          # 每批处理的字幕数量
    thread_num: 10                           # 并发线程数
    custom_prompt: "fubuki"                       # 自定义提示词文件名（相对于 vat/llm/prompts/custom/），如 "translate/example.md"，空字符串表示不使用
    enable_context: true                    # 是否启用前文上下文（新增）
    
    # Optimize 配置（嵌套）- 字幕优化
    optimize:
      enable: true                          # 是否启用字幕优化（修正错别字、统一术语）
      custom_prompt: "fubuki"                     # 优化自定义提示词文件名（相对于 vat/llm/prompts/custom/），如 "optimize/example.md"，空字符串表示不使用
      model: "gemini-3-flash-preview"  #"gpt-5-nano"                     # 优化模型（比翻译便宜，术语纠错靠 prompt 引导）
      # api_key: "${VAT_RESELLER_APIKEY}"       # 中转站 API Key
      # base_url: "https://api.videocaptioner.cn" # 中转站 Base URL
      # batch_size: 0                         # 可选覆写批大小（0=继承父级 translator.llm.batch_size）
      # thread_num: 0                         # 可选覆写线程数（0=继承父级 translator.llm.thread_num）
  
  
  # 本地模型配置
  local:
    model_filename: "translator/sakura-14b.gguf"  # 【模型路径】相对于 storage.models_dir 的路径
                                            # 完整路径: storage.models_dir/translator/sakura-14b.gguf
                                            # 需要手动下载 GGUF 格式模型并放置到此位置
    backend: "sakura-010"                   # 后端：sakura-009/sakura-010/galtransl
    n_gpu_layers: 35                        # GPU层数（根据显存调整）
    context_size: 4096                      # 上下文大小
    

# =============================================================================
# 字幕嵌入配置
# =============================================================================
embedder:
  subtitle_formats: ["srt", "ass"]          # 生成的字幕格式
  
  # 字幕嵌入模式
  embed_mode: "hard"                        # soft=软字幕（推荐，快速且保持原画质）
                                            # hard=硬字幕（烧录到画面，兼容性好）
  
  output_container: "mp4"                   # 输出容器格式
                                            # mkv: 推荐，支持完整ASS样式
                                            # mp4: 兼容性好，但会丢失ASS样式
  
  # ===== 以下参数仅在 embed_mode="hard" 时使用 =====
  video_codec: "libx265"                    # 视频编码器：libx265=H.265, libx264=H.264
                                            # GPU加速时自动切换：libx265→hevc_nvenc, libx264→h264_nvenc
  audio_codec: "copy"                       # 音频编码器（copy=不重新编码，节省时间和保持质量）
  crf: 28                                   # 质量参数（H.265推荐26-30，H.264推荐20-26）
  preset: "medium"                          # CPU编码预设（ultrafast/fast/medium/slow）
  use_gpu: true                             # GPU加速（强烈推荐！H.265速度提升3-5x，文件更小）
  
  # 字幕样式配置
  subtitle_style: "default"                 # 样式模板名称（对应 resources/subtitle_style/*.txt）
                                            # 可选值: default, 毕导科普风, 番剧可爱风, 竖屏
  
  # NVENC 并发控制
  max_nvenc_sessions_per_gpu: 5             # 每张 GPU 最大并发 NVENC 编码会话数
                                            # RTX 消费级显卡硬件限制通常为 5（可通过补丁解除）
                                            # 超出限制的任务会排队等待，而非直接失败
  


# =============================================================================
# 上传器配置
# =============================================================================
uploader:
  bilibili:
    # 连接设置（上传投稿内容设置见 config/upload.yaml，支持 Web UI 在线编辑）
    cookies_file: "cookies/bilibili/account.json"  # Cookie文件路径（获取方式: vat bilibili login 或 Web UI）
    line: "AUTO"                            # 上传线路：AUTO/bda2/qn/ws
    threads: 3                              # 上传线程数
    
# =============================================================================
# GPU 配置
# =============================================================================
gpu:
  device: "auto"                            # GPU 设备选择策略
                                            # "auto": 自动选择显存占用最低的 GPU
                                            # "cuda:N": 使用指定 GPU (N = 0, 1, 2, ...)
                                            # "cpu": 显式使用 CPU（不推荐，违反 GPU 原则）
  allow_cpu_fallback: false                 # 是否允许 CPU 回退（默认 false，遵循 GPU 原则）
  min_free_memory_mb: 8000                  # 自动选择时的最小空闲显存要求 (MB)

# =============================================================================
# 并发配置
# =============================================================================
concurrency:
  gpu_devices: [0]                    # 可用GPU列表（多 GPU 调度时使用）
  max_concurrent_per_gpu: 1                 # 每个GPU同时处理的视频数

# =============================================================================
# 日志配置
# =============================================================================
logging:
  level: "INFO"                             # 日志级别： DEBUG/INFO/WARNING/ERROR
  file: "vat.log"                          # 日志文件
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# =============================================================================
# LLM配置（统一管理，用于断句、翻译、视频信息翻译等所有LLM调用）
# =============================================================================
llm:
  api_key: "${VAT_GOOGLE_APIKEY}"            # Google API Key（全局默认）
  base_url: "https://generativelanguage.googleapis.com/v1beta/openai"  # Google API Base URL
  model: "gemini-3-flash-preview"                  # 全局默认模型（各阶段未指定 model 时的 fallback）

# =============================================================================
# 代理配置（全局默认 + 各环节独立覆盖）
# =============================================================================
proxy:
  http_proxy: "http://localhost:7890"       # 全局默认代理（用于下载器、HuggingFace 模型加载、LLM API 等）
                                            # 留空字符串 "" 表示不使用代理
  # 各环节代理覆盖（留空或不设置 = 使用全局 http_proxy）
  # 解析优先级：环节专属 → llm（LLM环节共用） → http_proxy
  downloader: "http://localhost:7790"                          # 下载器（YouTube 访问）
  # llm: ""                                 # 所有 LLM API 调用的默认代理
  translate: "http://localhost:7990"                           # 翻译 LLM（如 Google API 需特定地区代理）
  # optimize: ""                            # 优化 LLM（fallback: translate → llm → http_proxy）
  # split: ""                               # 断句 LLM
  # scene_identify: ""                      # 场景识别 LLM
  # video_info_translate: ""                # 视频信息翻译 LLM

# =============================================================================
# Web UI 配置
# =============================================================================
web:
  host: "0.0.0.0"                            # 监听地址（0.0.0.0 表示所有网卡）
  port: 13579                                 # 监听端口
