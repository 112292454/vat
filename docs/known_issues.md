# VAT 已知问题

本文档记录**已知但尚未修复**的问题，以及 LLM 成本参考信息。已修复的问题归档在 [docs/archive/](archive/) 目录。

---

## 一、LLM 成本策略

### 当前方案

各阶段按重要性分配不同模型，控制成本：

| 阶段 | 模型 | 相对成本 | 说明 |
|------|------|---------|------|
| Split | gpt-4o-mini | 1x | 断句对模型要求低，后有时间戳修正兜底 |
| Optimize | gpt-5-nano | 6x | 术语纠错靠 custom prompt 引导 |
| Translate | gemini-3-flash | 40x（开 reflect 约 60-80x） | 翻译质量要求最高，直接面向用户 |

### 效果对比

- **gpt-4o-mini 全流程**：明显不通顺，不可用于正式翻译
- **gpt-5-nano（optimize+translate）**：基本通顺，但专有名词误报多，缺乏个人风格
- **gemini-3-flash 全流程**：接近人工翻译质量，术语处理正确，风格自然

### Reflect 开销说明

Reflect 不是多次 LLM 调用，而是单次调用中输出 3 个字段（initial_translation + reflection + native_translation）。
实际开销增量：output token 约 2.5-3x，综合成本约 1.5-2x（取决于 batch_size 对 prompt 开销的摊薄）。

---

## 二、ASR（语音识别）

### ASR-1: 偶发漏句

- **现象**：频率很低，但会漏掉整句
- **原因**：faster-whisper 本身的局限
- **状态**：暂无解决方案，等待上游改进

### ASR-2: BGM/歌唱场景识别差

- **现象**：BGM 较大的歌唱、音调明显偏离正常讲话的语调，ASR 会漏检
- **补充**：一旦 ASR 能识别到，翻译效果依然理想
- **状态**：暂无解决方案。可考虑 two-pass 方案（常规识别 + 歌唱专用参数），但开发成本高

### ASR-3: 多讲话人问题

- **现象**：多人同时讲话时 ASR 识别质量下降
- **状态**：暂无解决方案。测试过 kotoba-whisper + diarizers，效果不如 faster-whisper large-v3

### ASR-4: 漏字、错字与同音异字

- **现象**：日语场景尤其严重——片假名、平假名、汉字三种写法读音可能相同但含义不同
- **影响**：下游 optimize 阶段的 diff 校验可能将合法的同义替换误判为非法修改。已通过片假名→平假名归一化缓解
- **状态**：部分缓解，无法完全解决

### ASR-5: 幻觉输出

- **现象**：VTuber 直播无声序幕（只有画面、无语音）容易产生幻觉文本
- **缓解**：默认关闭 `condition_on_previous_text`，防止幻觉蔓延
- **状态**：已有后处理检测（幻觉检测模块），但无法 100% 消除

### ASR-6: initial_prompt 无通用有效方案

- **现象**：测试了多种写法，绝大部分情况效果变差
- **状态**：暂不使用。详见 [ASR 参数指南](asr_parameters_guide.md)

---

## 三、Split（智能断句）

### Split-1: 断句后偶发时间错位

- **现象**：经 split 后字幕偶尔出现轻微时间错位（延后），但无法稳定复现
- **状态**：已修复主要问题（字符级时间插值 + 重叠消除），不排除边缘情况仍有微小偏差

---

## 四、Translate（翻译）

### Translate-1: chunk 间上下文传递

- **现象**：翻译阶段按 chunk 并行处理，chunk 之间无上下文传递（Optimize 阶段已改为带上下文的线性处理）
- **状态**：理论上添加 chunk 间上下文可提升连贯性，但会将并行改为串行。收益与成本待评估

---

## 五、B 站上传

### Upload-1: 添加合集不稳定

- **现象**：视频上传成功，但添加到合集有时会失败
- **状态**：已知问题，暂未定位根因
