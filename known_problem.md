现在的已知问题绝大部分集中在asr阶段。但是也基本都是老生常谈的问题，没有什么新意。所以我也没有什么很好的办法，只能是参考了几个仓库，并且做了一些参数的测试集成。最后效果在经过了大模型的优化之后，看起来还可以，所以我暂时将这个还是发布出来。

比如首先的一个问题就是识别的时候经常有漏字，错字同音，但不同字的情况发生。而这个问题在日语情景下变本加厉，因为日语的片假名，平假名，汉字这三个东西读音是一样的，但是含义却有可能不一样。而写法的字符更是经常不一样，这就对上层的一些操作带来了极大的困难。比如说在optimize阶段这三种写法之间的同义替换应该是完全合法的，甚至有的时候大模型是可以把一些纯发音的片假名识别成有意义的汉字，但是却会被目前的检测方法(char diff)给判定为不合法，打回去重新修改。
——对游戏直播的情况就是那种可能声音不是一个完整的句子，可能就是一些这个感叹呀，拟声词呀，喊叫声呀这之类的，这种情况的漏字会格外的明显。

所以这个主要有几种表现就是是以及上述的日语写法问题。不过后者如果是处理非日语的音频应该会好很多。

然后就是很经典的AR幻觉问题。就我的测试来看，感觉在正常的连贯的对话或者正常的语音中的时候，幻觉是不太容易发生的。主要问题在于大部分的这个virtual YouTube的直播，它的开头会有一些类似于这个序幕一样，就是没有声音，只有一些画面。的阶段。这个东西就很恶心，它很容易被asr识别出幻觉。且如果在配置里面不关闭那个condition on on previous text,这个幻觉还会向下蔓延，让后面的很长一段时间大概1~3分钟的内容全部被毁掉.所以condition on previous text这个看起来应该是会增强的功能，我在默认配置里其实是把它关掉的。

然后这个fast Whisper他支持一个这个initial promote，这个东西我不知道它的用法，毕竟我也我也不是专业搞这个领域的。我科研主要还是搞纯AI的东西。这个东西我测试了一下，就是不管我怎么填都不能达到一个。General battle的效果。包括使用英文的提示，使用日文的提示，使用中文的提示以及各种内容，比如说你是一个YouTube主播，你是一个这个日本主播。啊，你在主播一个什么主题。都是绝大部分情况下会变差。甚至导致他重复这个initial promote，只有少部分情况下可能会变。变好上面这些关于这个参数的具体详情可以参考我们。对于这个asr参数分析的文档对

这部分可能也可以考虑那种two pass的处理，比如说我现在这个配置对于bgm比较大的语言，比较这个波动比较大的这这种唱歌的片段。识别效果是非常差的。基本上那个词都是大模型，自己rewrite出来的。所以是不是可以做一个双面的识别第一遍就是现在的这个状态？找出大部分的字幕，然后调一个适合用于歌词做识别的模型。或者参数来对这其中的片段去做识别的。不过这个东西一来是时间开销大大增加，二来是开发成本也会大大增加。并且这个东西不是我的主要目标。所以暂时搁置，只在这里做一个提议




那么在这个阶段之前，这个下载的阶段整体上还是比较流畅的，只不过显而易见会受到YouTube的风控。那么这其中有几个需要注意的点也算是一个提示吧，这个不不算是一个问题。就是使用YouTube lp获取playlist里面的视频列表的时候，这个阶阶段是没有什么风控的，对逐个视频获取它的info的时候会有一点，比如说你的并发开到10左右。就会偶尔遇上他要你做那个check，但是这个东西能被目前最新版的YouTube dp所自动处理，所以也不用管。我目前的默认配置就是把这个获取音符的操作并发至10。但是在下载视频内容的时候，这个YouTube的风控好像还是比较明显的，很容易就会遇见429或者401之类的东西。所以我建议我做的那个并发的处理。放在后面的我们自己的阶段，在下载的阶段还是让这个并发=1


然后接着往下讲这个分词的阶段，也就是这个split sentence的阶段。这个阶段目前可能有一个问题，就是我在翻译某些视频的时候观察到这个ass rt提出来的字幕，虽然他可能有错，但是时间是准确的。字幕和语音是对得上的，经过split之后可能会有一定的延后的错位情况。但是这个我没有稳定的复现。并且这个显而易见，应该是代码的问题。他在处理这个character level timestamp的时候，没有准确的对齐。等好心人排查修复一下。
——这个地方我说不能稳定复现的意思就是asr和这个地方的分割好像都是会变化的，比如说在我排查这个问题的时候，我我就试了几次。有的时候这个asr他妈缺字。有的时候又又正常，然后这个分词有的时候分的没有任何问题，有的时候又有时间差。就很奇怪。——这个现在有一定的修复，就是之前那个比较明显的问题是字符的时间戳对齐，确实确实不对，我修了一下。但是我直觉感觉这个地方的问题可能是一个持续性的，所以这里还是留着记录一下。
而这个分词的效果目前只能说能用，肯定还可以优化。比如说我现在只设了最大和最小的分距长度。那我是不是可以有？Recommend的长度是不是可以有这个scence specify的长度？

分词的时间戳对齐算法已改为基于 diff 的容错对齐（chunked_split.py `_realign_timestamps`）：使用 difflib.SequenceMatcher 找到原始ASR文本和LLM分句文本之间的匹配块，匹配字符直接取原始时间戳，小范围增删改（≤3字符）自动容错。同时 split.py 的验证逻辑要求规范化后100%字符一致（作为LLM反馈信号），并支持模型升级链（失败后自动尝试更强模型）。安全检查确保每个段落的时间不超出其字符所属原始ASR segment的范围。已在28个视频全量数据上测试通过。

此外修复了分块合并（chunk overlap）导致文本丢失的 bug：旧逻辑在合并相邻 chunk 时按固定段数（`segments[overlap:]`）跳过 overlap 部分，但 LLM 可能将 overlap segment 的文本与后续文本合并成一个 split segment，导致跳过该 segment 时连带丢失后续文本。实测 9bSJy5Byrfc（2841 segments, 58 chunks）旧方法丢失 998 字符。修复为基于字符计数的精确 overlap 移除（`_trim_overlap`），支持跨边界 segment 拆分，模拟测试丢失 0 字符。



那往后到了这个字幕优化的阶段。好这个阶段吧就比较简单，也没有什么会造成问题的阶段。这个阶段可能唯一需要注意的就是它的效果可以被custom promote所影响所以说如果我们是针对某个人的系列视频做处理的时候，最好自己写一些规则，然后丢给chat让他参考我之前写好的。文档或者让他自由发挥去完善一下custom prompt。这个对效果是有明显影响的。因为很多专有的称呼和和术语。都是需要人为来补充的。

然后翻译阶段也是也是类似，都要重视这个customer t。只不过这个地方我不知道是否应该修改的一个问题就是我参考的那个video captioning原本的实现是把所有字幕分chunk之间并行处理。那么这显然是不能给唱歌添加上下文的。所以我在前面的optimize阶段做了基于上下文的翻译。那么这显然就变成了一个顺序线性的范围。我初步的考虑是这个optimize对上下文的依赖要求可能比较高。所以就把它改成了线性，至于这个翻译阶段。理论上肯定如果添加上下文那是会变好的，但是这个变好的收益值不值得去改？存疑所以我这里暂时先没有改，不过我觉得这个很有可能是值得换成带上下文的。