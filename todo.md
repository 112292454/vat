## 已完成

- [x] 翻译场景提示词优化 - 已完成 split/optimize/translate 的场景提示词


- [x] ASR 识别效果优化
  - 已完成：temperature=0.0 固定、no_speech_threshold=0.6、后处理清理幻觉
  - 测试结论：人声分离对游戏直播反而有害，默认关闭





- 失败时的正确标识——现在很多阶段失败之后都有对flag的处理，但是非常不完善，经常误报。我之前都是人工判断的。以及恢复更细粒度（比如asr和translate实际上都是两段任务，但是现在不能单段标识或者处理——我们不改变pipeline的环节数目，但是这些sub phase在有必要的时候还是要能处理的）、更便于调试（各种人类友好的问题，包括输出、数据检查等等）
- 与其联动的问题还有：现在的结构实在是太庞大，太复杂了。executor的实现应该遵循良好的设计模式做简化，要一目了然。仅将不得不在这里胶水代码保留。对于可以重构集成到各个环节组件的代码，应该恰当的重构集成。而各个组件里面现在也很臃肿：未使用的代码、过度的包装、过度的耦合或者抽象、不良的功能划分、多个版本的遗留代码混合（比如GalTransl特有的代码和组件就应该被完全移除，我们现在用不着他了。多speaker、whisper的Transformer pipeline形式的代码分支也应该抽取出去到单独的文件，因为我们现在实际上并没有实现，但是后面可能集成这个功能，所以现在应该把它移出去，只保留接口调用，眼不见心不烦）等等

- [x] 已完成：代码结构清理、GalTransl 移除、pipeline 代码分离整理



- [x] GPU 自动选择 - 已完成：whisper/ffmpeg/人声分离等都会自动选择占用率最低的 GPU


- [ ] translate阶段没有进度输出（低优先级）

## 待完成

- [ ] Upload 模块集成（使用 biliup 作为胶水层）






## 多说话人上下文优化

26.1.25update: 无限期搁置：因为这个多说话人识别功能是基于那个kotoba whisper模型的，它用了 diarizers 
但是他写的太烂了  代码有bug  我修完给他提上去  然后一跑 看效果 发现还tm不如fast whisper large3的
这还跑个几把  不用它了  这个多讲话人也暂缓吧  浪费我一下午一晚上 

### 背景
当前实现中，Split和翻译按说话人独立处理，每个说话人的上下文是孤立的。
在VTuber联动场景中，不同说话人之间的对话存在关联性，如果能在翻译时提供完整上下文，可能提升翻译质量。

### 潜在改进方案
1. **Split保持独立，翻译提供全局上下文**
   - Split仍按说话人独立处理（避免不同说话人的句子被合并）
   - 翻译时提供完整对话历史作为参考上下文
   - 示例prompt：`请翻译Speaker_01的以下内容，参考对话历史：[Speaker_00: xxx] [Speaker_01: yyy] ...`

2. **智能上下文窗口**
   - 为每个待翻译片段提供时间上相邻的N条对话作为上下文（不限说话人）
   - 使用滑动窗口策略（如前后各3句）

3. **后处理术语统一**
   - 在所有说话人翻译完成后，识别对话中的关键术语（人名、地名、游戏术语等）
   - 使用LLM进行跨说话人的术语统一后处理

### 评估要点
- 上下文引入的token成本增加
- 翻译质量提升是否明显
- 是否会引入误翻（过度依赖上下文）

### 优先级
中等（先验证当前方案效果，再决定是否需要此优化）