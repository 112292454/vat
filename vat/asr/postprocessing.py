"""
ASR åå¤„ç†æ¨¡å—ï¼šå¹»è§‰æ£€æµ‹ã€é‡å¤æ¸…ç†ã€æ—¥è¯­ç‰¹æ®Šå¤„ç†

å€Ÿé‰´è‡ª WhisperJAV é¡¹ç›®ï¼Œé’ˆå¯¹ VTB ç›´æ’­åœºæ™¯ä¼˜åŒ–
"""
import re
from typing import List, Dict, Any, Tuple, Optional, Set
from dataclasses import dataclass, field
from collections import Counter

from vat.utils.logger import setup_logger

logger = setup_logger("asr_postprocessing")


# ============================================================================
# å¹»è§‰æ£€æµ‹å¸¸é‡
# ============================================================================

# æ—¥è¯­å¸¸è§å¹»è§‰æ–‡æœ¬ï¼ˆå®Œæ•´è¡ŒåŒ¹é…ï¼‰
JAPANESE_HALLUCINATION_EXACT: Set[str] = {
    # å¸¸è§è¯¯è¯†åˆ«
    'www', 'wwww', 'wwwww', 'ï½—ï½—ï½—', 'ï½—ï½—ï½—ï½—',
    'ok', 'OK', 'Ok',
    'ç¬‘', 'ï¼ˆç¬‘ï¼‰', '(ç¬‘)',
    'w', 'ï½—',
    # ç»“æŸè¯­å¹»è§‰
    'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
    'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™', 
    'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
    'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²ãŠé¡˜ã„ã—ã¾ã™',
    'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™',
    'ã‚°ãƒƒãƒ‰ãƒœã‚¿ãƒ³ãŠé¡˜ã„ã—ã¾ã™',
    'é«˜è©•ä¾¡ãŠé¡˜ã„ã—ã¾ã™',
    # å­—å¹•ç›¸å…³å¹»è§‰
    'å­—å¹•',
    'å­—å¹•ï¼š',
    'ç¿»è¨³ï¼š',
    'ç·¨é›†ï¼š',
    # éŸ³æ•ˆæè¿°ï¼ˆé€šå¸¸æ˜¯è¯¯è¯†åˆ«ï¼‰
    'â™ª',
    'â™ªâ™ª',
    'â™ªâ™ªâ™ª',
    'ğŸµ',
    # ç©ºç™½/æ— æ„ä¹‰
    '...',
    'â€¦',
    'ã€‚ã€‚ã€‚',
    'ã€ã€ã€',
}

# å¹»è§‰æ­£åˆ™æ¨¡å¼
HALLUCINATION_REGEX_PATTERNS: List[Tuple[str, str, float]] = [
    # (pattern, category, confidence)
    (r'^(OK|ok|Ok)+$', 'common_hallucination', 1.0),
    (r'^[wWï½—ï¼·]+$', 'common_hallucination', 1.0),
    (r'^ç¬‘+$', 'common_hallucination', 1.0),
    (r'^(ã”|ãŠ)?è¦–è´.*ã‚ã‚ŠãŒã¨ã†.*$', 'closing_phrase', 0.95),
    (r'^ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ².*$', 'closing_phrase', 0.95),
    (r'^[â™ªğŸµğŸ¶]+$', 'music_symbol', 0.9),
    (r'^[\.\â€¦ã€‚ã€]+$', 'punctuation_only', 1.0),
    # æ‹¬å·åŒ…è£¹çš„æè¿°æ€§æ–‡æœ¬
    (r'^[\(ï¼ˆ\[ã€ã€Œã€ã€Š].*[\)ï¼‰\]ã€‘ã€ã€ã€‹]$', 'bracketed_context', 0.8),
]

# æ—¥è¯­é‡å¤æ¨¡å¼æ¸…ç†
REPETITION_PATTERNS: List[Tuple[str, str, str]] = [
    # (name, pattern, replacement)
    # æç«¯çŸ­è¯­é‡å¤ï¼ˆå¸¦åˆ†éš”ç¬¦ï¼‰ï¼šã‚!!ã‚!!ã‚!! -> ã‚!!
    ('phrase_with_separator', r'((?:[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]{1,8}[ã€,!\sï¼ï¼ï¼Ÿï¼Ÿã€‚ã€‚ãƒ»]+))\1{3,}', r'\1'),
    # å¤šå­—ç¬¦è¯é‡å¤ï¼šãƒãƒƒãƒãƒƒãƒãƒƒãƒãƒƒ -> ãƒãƒƒãƒãƒƒ
    ('multi_char_word', r'((?:[\u3040-\u309f\u30a0-\u30ff]{2,4}))\1{3,}', r'\1\1'),
    # é€—å·çŸ­è¯­é‡å¤ï¼šã‚†ãƒ¼ã¡ã‚ƒã‚“ã€ã‚†ãƒ¼ã¡ã‚ƒã‚“ã€ã‚†ãƒ¼ã¡ã‚ƒã‚“ -> ã‚†ãƒ¼ã¡ã‚ƒã‚“ã€
    ('phrase_with_comma', r'((?:[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]{1,10}[ã€,]\s*))\1{2,}', r'\1'),
    # å•å­—ç¬¦æ´ªæ°´ï¼šã†ã†ã†ã†ã† -> ã†ã†
    ('single_char_flood', r'([\u3040-\u309f\u30a0-\u30ff])\1{3,}', r'\1\1'),
    # å‰ç¼€+å­—ç¬¦é‡å¤ï¼šã‚ã‚‰ã‚‰ã‚‰ã‚‰ -> ã‚ã‚‰ã‚‰
    ('prefix_plus_char', r'([\u3040-\u309f\u30a0-\u30ff]{1,2})([\u3040-\u309f\u30a0-\u30ff])\2{3,}', r'\1\2\2'),
    # å…ƒéŸ³å»¶é•¿ï¼šã‚ã€œã€œã€œã€œ -> ã‚ã€œã€œ
    ('vowel_extension', r'([\u3040-\u309f\u30a0-\u30ff])([ã€œãƒ¼])\2{3,}', r'\1\2\2'),
]

# æ—¥è¯­å¥å°¾åŠ©è¯ï¼ˆç”¨äºæ–­å¥ä¼˜åŒ–ï¼‰
JAPANESE_SENTENCE_ENDINGS = {
    'ã­', 'ã‚ˆ', 'ã‚', 'ã®', 'ã•', 'ãª', 'ã', 'ãœ', 'ã‹ãª', 'ã‹ã­',
    'ã‚ˆã­', 'ã‚ã­', 'ã®ã­', 'ã ã­', 'ã§ã™ã­', 'ã¾ã™ã­',
    'ã‚ˆãª', 'ã ãª', 'ã‹ãª', 'ã®ã‹ãª',
}

# æ—¥è¯­ç›¸æ§Œï¼ˆä¸åº”è¢«åˆ é™¤çš„çŸ­å›åº”ï¼‰
JAPANESE_AIZUCHI = {
    'ã†ã‚“', 'ã†ã†ã‚“', 'ã¯ã„', 'ãˆãˆ', 'ã‚ãƒ¼', 'ãˆãƒ¼', 'ãŠãƒ¼',
    'ãã†', 'ãã†ãã†', 'ãªã‚‹ã»ã©', 'ã¸ãƒ¼', 'ã»ãƒ¼', 'ãµãƒ¼ã‚“',
    'ã¾ã‚', 'ã¾ã‚ã¾ã‚', 'ã‚„ã£ã±ã‚Š', 'ã‚„ã£ã±',
    'ã¡ã‚‡ã£ã¨', 'ãˆã£ã¨', 'ã‚ã®ãƒ¼', 'ãˆãƒ¼ã£ã¨',
}


# ============================================================================
# æ•°æ®ç±»
# ============================================================================

@dataclass
class PostProcessingResult:
    """åå¤„ç†ç»“æœ"""
    original_text: str
    processed_text: str
    is_hallucination: bool = False
    modifications: List[Dict[str, Any]] = field(default_factory=list)
    
    @property
    def was_modified(self) -> bool:
        return self.original_text != self.processed_text


@dataclass
class PostProcessingStats:
    """åå¤„ç†ç»Ÿè®¡"""
    total_segments: int = 0
    hallucinations_removed: int = 0
    repetitions_cleaned: int = 0
    empty_removed: int = 0
    
    def to_dict(self) -> Dict[str, int]:
        return {
            'total_segments': self.total_segments,
            'hallucinations_removed': self.hallucinations_removed,
            'repetitions_cleaned': self.repetitions_cleaned,
            'empty_removed': self.empty_removed,
        }


# ============================================================================
# å¹»è§‰æ£€æµ‹å™¨
# ============================================================================

class HallucinationDetector:
    """
    å¹»è§‰æ£€æµ‹å™¨ï¼šè¯†åˆ«å¹¶ç§»é™¤ Whisper å¸¸è§çš„å¹»è§‰è¾“å‡º
    
    å¹»è§‰ç±»å‹ï¼š
    1. å®Œæ•´è¡ŒåŒ¹é…ï¼ˆå¦‚ "www", "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ"ï¼‰
    2. æ­£åˆ™æ¨¡å¼åŒ¹é…ï¼ˆå¦‚é‡å¤çš„æ ‡ç‚¹ã€æ‹¬å·åŒ…è£¹çš„æè¿°ï¼‰
    3. é«˜ç½®ä¿¡åº¦è¿‡æ»¤ï¼ˆåŸºäºæ–‡æœ¬ç‰¹å¾ï¼‰
    """
    
    def __init__(
        self,
        exact_matches: Optional[Set[str]] = None,
        regex_patterns: Optional[List[Tuple[str, str, float]]] = None,
        min_confidence: float = 0.8,
        custom_blacklist: Optional[List[str]] = None,
    ):
        """
        åˆå§‹åŒ–å¹»è§‰æ£€æµ‹å™¨
        
        Args:
            exact_matches: ç²¾ç¡®åŒ¹é…çš„å¹»è§‰æ–‡æœ¬é›†åˆ
            regex_patterns: æ­£åˆ™æ¨¡å¼åˆ—è¡¨ [(pattern, category, confidence), ...]
            min_confidence: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
            custom_blacklist: ç”¨æˆ·è‡ªå®šä¹‰é»‘åå•
        """
        self.exact_matches = exact_matches or JAPANESE_HALLUCINATION_EXACT
        self.regex_patterns = regex_patterns or HALLUCINATION_REGEX_PATTERNS
        self.min_confidence = min_confidence
        self.custom_blacklist = set(custom_blacklist) if custom_blacklist else set()
        
        # åˆå¹¶è‡ªå®šä¹‰é»‘åå•
        self.exact_matches = self.exact_matches | self.custom_blacklist
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compiled_patterns = [
            (re.compile(pattern), category, confidence)
            for pattern, category, confidence in self.regex_patterns
        ]
        
        logger.debug(f"HallucinationDetector åˆå§‹åŒ–: {len(self.exact_matches)} ç²¾ç¡®åŒ¹é…, "
                     f"{len(self._compiled_patterns)} æ­£åˆ™æ¨¡å¼")
    
    def detect(self, text: str) -> Tuple[bool, Optional[Dict[str, Any]]]:
        """
        æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºå¹»è§‰
        
        Args:
            text: å¾…æ£€æµ‹æ–‡æœ¬
            
        Returns:
            (is_hallucination, match_info)
        """
        if not text or not text.strip():
            return True, {'type': 'empty', 'confidence': 1.0}
        
        normalized = text.strip().lower()
        
        # 1. ç²¾ç¡®åŒ¹é…
        if normalized in self.exact_matches or text.strip() in self.exact_matches:
            return True, {
                'type': 'exact_match',
                'pattern': normalized,
                'confidence': 1.0,
                'category': 'hallucination',
            }
        
        # 2. æ­£åˆ™åŒ¹é…
        for compiled_pattern, category, confidence in self._compiled_patterns:
            if confidence < self.min_confidence:
                continue
            if compiled_pattern.match(text.strip()):
                return True, {
                    'type': 'regex_match',
                    'pattern': compiled_pattern.pattern,
                    'confidence': confidence,
                    'category': category,
                }
        
        # 3. æ‹¬å·åŒ…è£¹æ£€æµ‹ï¼ˆæè¿°æ€§æ–‡æœ¬ï¼‰
        bracket_info = self._check_bracketed(text.strip())
        if bracket_info:
            return True, bracket_info
        
        return False, None
    
    def _check_bracketed(self, text: str) -> Optional[Dict[str, Any]]:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ‹¬å·åŒ…è£¹çš„æè¿°æ€§æ–‡æœ¬"""
        bracket_pairs = [
            ('(', ')'), ('ï¼ˆ', 'ï¼‰'),
            ('[', ']'), ('ï¼»', 'ï¼½'),
            ('{', '}'), ('ï½›', 'ï½'),
            ('ã€', 'ã€‘'), ('ã€', 'ã€'),
            ('ã€Œ', 'ã€'), ('ã€Š', 'ã€‹'),
        ]
        
        for left, right in bracket_pairs:
            if text.startswith(left) and text.endswith(right):
                inner = text[len(left):-len(right)].strip()
                # å¦‚æœå†…éƒ¨æ˜¯æè¿°æ€§æ–‡æœ¬ï¼ˆå¦‚ "æ‹æ‰‹"ã€"ç¬‘å£°" ç­‰ï¼‰
                if inner and len(inner) <= 10:
                    return {
                        'type': 'bracketed_context',
                        'pattern': f'{left}...{right}',
                        'confidence': 0.85,
                        'category': 'context_caption',
                        'inner_text': inner,
                    }
        return None
    
    def is_valid_japanese_content(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ—¥è¯­å†…å®¹ï¼ˆé¿å…è¯¯åˆ ï¼‰
        
        ä¿æŠ¤è§„åˆ™ï¼š
        - åŒ…å«æ±‰å­—+å‡åæ··åˆ
        - åŒ…å«å¸¸è§å¥å¼ç»“æ„
        - æ˜¯ç›¸æ§Œï¼ˆçŸ­å›åº”ï¼‰
        """
        text = text.strip()
        
        # ç›¸æ§Œä¿æŠ¤
        if text in JAPANESE_AIZUCHI:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥è¯­ç‰¹å¾
        has_hiragana = bool(re.search(r'[\u3040-\u309f]', text))
        has_katakana = bool(re.search(r'[\u30a0-\u30ff]', text))
        has_kanji = bool(re.search(r'[\u4e00-\u9fff]', text))
        
        # æ··åˆè„šæœ¬é€šå¸¸æ˜¯æœ‰æ•ˆå†…å®¹
        script_count = sum([has_hiragana, has_katakana, has_kanji])
        if script_count >= 2:
            return True
        
        # åŒ…å«å¸¸è§æ—¥è¯­è¯­æ³•ç»“æ„
        grammar_markers = ['ã§ã™', 'ã¾ã™', 'ã ', 'ã§ã‚ã‚‹', 'ã§ã—ãŸ', 'ã¾ã—ãŸ', 
                          'ã„ã‚‹', 'ã‚ã‚‹', 'ã™ã‚‹', 'ã—ãŸ', 'ã£ã¦', 'ã¨ã„ã†']
        if any(marker in text for marker in grammar_markers):
            return True
        
        # åŒ…å«æ•°å­—æˆ–è´§å¸
        if re.search(r'[\dÂ¥$â‚¬Â£å††]', text):
            return True
        
        return False


# ============================================================================
# é‡å¤æ¸…ç†å™¨
# ============================================================================

class RepetitionCleaner:
    """
    é‡å¤æ¸…ç†å™¨ï¼šæ¸…ç† Whisper è¾“å‡ºä¸­çš„å¼‚å¸¸é‡å¤
    
    å¤„ç†æ¨¡å¼ï¼š
    - å­—ç¬¦æ´ªæ°´ï¼ˆå¦‚ "ã†ã†ã†ã†"ï¼‰
    - çŸ­è¯­é‡å¤ï¼ˆå¦‚ "ãƒãƒƒãƒãƒƒãƒãƒƒ"ï¼‰
    - æ ‡ç‚¹é‡å¤
    """
    
    def __init__(
        self,
        patterns: Optional[List[Tuple[str, str, str]]] = None,
        threshold: int = 2,  # ä¿ç•™çš„æœ€å¤§é‡å¤æ¬¡æ•°
    ):
        """
        åˆå§‹åŒ–é‡å¤æ¸…ç†å™¨
        
        Args:
            patterns: æ¸…ç†æ¨¡å¼åˆ—è¡¨ [(name, pattern, replacement), ...]
            threshold: é‡å¤é˜ˆå€¼
        """
        self.patterns = patterns or REPETITION_PATTERNS
        self.threshold = threshold
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compiled_patterns = [
            (name, re.compile(pattern), replacement)
            for name, pattern, replacement in self.patterns
        ]
        
        logger.debug(f"RepetitionCleaner åˆå§‹åŒ–: {len(self._compiled_patterns)} æ¸…ç†æ¨¡å¼")
    
    def clean(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        æ¸…ç†æ–‡æœ¬ä¸­çš„é‡å¤
        
        Args:
            text: å¾…æ¸…ç†æ–‡æœ¬
            
        Returns:
            (cleaned_text, modifications)
        """
        if not text or not text.strip():
            return text, []
        
        modifications = []
        current_text = text
        
        for name, compiled_pattern, replacement in self._compiled_patterns:
            try:
                original = current_text
                new_text = compiled_pattern.sub(replacement, current_text)
                
                if new_text != original:
                    modifications.append({
                        'type': name,
                        'pattern': compiled_pattern.pattern,
                        'original': original,
                        'modified': new_text,
                        'category': 'repetition_cleaning',
                    })
                    current_text = new_text
                    
            except Exception as e:
                logger.warning(f"é‡å¤æ¸…ç†æ¨¡å¼ '{name}' å¤„ç†å¤±è´¥: {e}")
                continue
        
        return current_text.strip(), modifications
    
    def is_all_repetition(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦å‡ ä¹å…¨æ˜¯é‡å¤"""
        stripped = re.sub(r'[\s\u3000]', '', text)  # ç§»é™¤ç©ºç™½
        if len(stripped) < 10:
            return False
        
        # æ£€æŸ¥å•å­—ç¬¦å æ¯”
        char_counts = Counter(stripped)
        if char_counts:
            most_common_char, count = char_counts.most_common(1)[0]
            if count / len(stripped) > 0.8:
                return True
        
        return False


# ============================================================================
# æ—¥è¯­åå¤„ç†å™¨
# ============================================================================

class JapanesePostProcessor:
    """
    æ—¥è¯­åå¤„ç†å™¨ï¼šé’ˆå¯¹æ—¥è¯­ç‰¹æ€§çš„ä¼˜åŒ–å¤„ç†
    
    åŠŸèƒ½ï¼š
    - å¥å°¾åŠ©è¯å¤„ç†
    - ç›¸æ§Œè¯†åˆ«
    - æ–¹è¨€é€‚é…
    - VTB ç”¨è¯­å¤„ç†
    """
    
    # VTB å¸¸è§ç”¨è¯­ï¼ˆä¸åº”è¢«åˆ é™¤æˆ–ä¿®æ”¹ï¼‰
    VTB_TERMS = {
        'ã‚¹ãƒ‘ãƒãƒ£', 'ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ£ãƒƒãƒˆ', 'ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—', 'ãƒ¡ãƒ³é™',
        'ã‚³ãƒ©ãƒœ', 'ã‚²ãƒªãƒ©', 'æ ', 'é…ä¿¡', 'ç”Ÿæ”¾é€',
        'ã‚³ãƒ¡ãƒ³ãƒˆ', 'ãƒãƒ£ãƒƒãƒˆ', 'ãƒªã‚¹ãƒŠãƒ¼', 'è¦–è´è€…',
        'ã‚°ãƒƒã‚º', 'ã‚ªãƒªæ›²', 'ã‚«ãƒãƒ¼', 'æ­Œæ ', 'é›‘è«‡',
        'ã‚ããŸã‚“', 'ãã‚‰ã¡ã‚ƒã‚“', 'ã“ã‚ã­', 'ãºã“ã‚‰',  # å¸¸è§æ˜µç§°åç¼€
    }
    
    def __init__(self):
        self.sentence_endings = JAPANESE_SENTENCE_ENDINGS
        self.aizuchi = JAPANESE_AIZUCHI
        
        logger.debug("JapanesePostProcessor åˆå§‹åŒ–")
    
    def process(self, text: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        å¤„ç†æ—¥è¯­æ–‡æœ¬
        
        Args:
            text: å¾…å¤„ç†æ–‡æœ¬
            
        Returns:
            (processed_text, modifications)
        """
        if not text or not text.strip():
            return text, []
        
        modifications = []
        current_text = text
        
        # 1. å…¨è§’/åŠè§’æ ‡å‡†åŒ–
        normalized = self._normalize_punctuation(current_text)
        if normalized != current_text:
            modifications.append({
                'type': 'punctuation_normalization',
                'original': current_text,
                'modified': normalized,
            })
            current_text = normalized
        
        # 2. æ¸…ç†å¤šä½™ç©ºæ ¼
        cleaned = self._clean_whitespace(current_text)
        if cleaned != current_text:
            modifications.append({
                'type': 'whitespace_cleanup',
                'original': current_text,
                'modified': cleaned,
            })
            current_text = cleaned
        
        return current_text, modifications
    
    def _normalize_punctuation(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·"""
        # æ—¥è¯­å¸¸ç”¨å…¨è§’æ ‡ç‚¹
        replacements = [
            ('!', 'ï¼'),
            ('?', 'ï¼Ÿ'),
            (',', 'ã€'),
            # ä¿ç•™å¥å·çš„åŸæ ·ï¼ˆå¯èƒ½æ˜¯å…¨è§’æˆ–åŠè§’ï¼‰
        ]
        
        result = text
        for old, new in replacements:
            result = result.replace(old, new)
        
        return result
    
    def _clean_whitespace(self, text: str) -> str:
        """æ¸…ç†å¤šä½™ç©ºç™½
        
        æ—¥è¯­æ–‡æœ¬ä¸ä½¿ç”¨ç©ºæ ¼åˆ†è¯ï¼ŒASR æœ‰æ—¶ä¼šåœ¨æ¯ä¸ªè¯/å­—ä¹‹é—´æ’å…¥ç©ºæ ¼ï¼Œ
        ä¾‹å¦‚ "å¾®ç†± ã« ãªã£ ã¦ ã ãŸ ã‚“ ã  ã‘ã©"ï¼Œè¿™äº›ç©ºæ ¼æ˜¯æ— æ„ä¹‰çš„ã€‚
        
        ç­–ç•¥ï¼š
        - CJK å­—ç¬¦ä¹‹é—´çš„ç©ºæ ¼ï¼šç›´æ¥ç§»é™¤
        - CJK ä¸ ASCII ä¹‹é—´çš„ç©ºæ ¼ï¼šç§»é™¤ï¼ˆæ—¥è¯­å­—å¹•ä¸­ä¸éœ€è¦ï¼‰
        - ASCII å•è¯ä¹‹é—´çš„ç©ºæ ¼ï¼šä¿ç•™ï¼ˆå¦‚ "CLIP STUDIO PAINT"ï¼‰
        """
        if not text:
            return text
        
        # åˆ¤æ–­æ˜¯å¦ä¸º CJK ä¸ºä¸»çš„æ–‡æœ¬
        cjk_count = len(re.findall(r'[\u3000-\u9fff\uf900-\ufaff\uff00-\uffef]', text))
        total_alpha = len(re.findall(r'\S', text))
        
        if total_alpha > 0 and cjk_count / total_alpha >= 0.3:
            # CJK ä¸ºä¸»çš„æ–‡æœ¬ï¼šç§»é™¤ CJK å­—ç¬¦å‘¨å›´çš„ç©ºæ ¼
            # 1. ç§»é™¤ä¸¤ä¸ª CJK/kana å­—ç¬¦ä¹‹é—´çš„ç©ºæ ¼
            result = re.sub(
                r'(?<=[\u3000-\u9fff\uf900-\ufaff\uff00-\uffef])'
                r'[\s]+'
                r'(?=[\u3000-\u9fff\uf900-\ufaff\uff00-\uffef])',
                '', text
            )
            # 2. ç§»é™¤ CJK ä¸ ASCII ä¹‹é—´çš„ç©ºæ ¼
            result = re.sub(
                r'(?<=[\u3000-\u9fff\uf900-\ufaff\uff00-\uffef])[\s]+(?=[A-Za-z0-9])',
                '', result
            )
            result = re.sub(
                r'(?<=[A-Za-z0-9])[\s]+(?=[\u3000-\u9fff\uf900-\ufaff\uff00-\uffef])',
                '', result
            )
        else:
            # é CJK æ–‡æœ¬ï¼šåªåˆå¹¶è¿ç»­ç©ºæ ¼
            result = re.sub(r'[ \t]+', ' ', text)
        
        return result.strip()
    
    def is_aizuchi(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç›¸æ§Œ"""
        return text.strip() in self.aizuchi
    
    def has_sentence_ending(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¥å°¾åŠ©è¯"""
        text = text.strip()
        for ending in self.sentence_endings:
            if text.endswith(ending):
                return True
        return False


# ============================================================================
# ç»¼åˆåå¤„ç†å™¨
# ============================================================================

class ASRPostProcessor:
    """
    ASR ç»¼åˆåå¤„ç†å™¨
    
    æ•´åˆå¹»è§‰æ£€æµ‹ã€é‡å¤æ¸…ç†ã€æ—¥è¯­å¤„ç†çš„å®Œæ•´æµç¨‹
    """
    
    def __init__(
        self,
        enable_hallucination_detection: bool = True,
        enable_repetition_cleaning: bool = True,
        enable_japanese_processing: bool = True,
        custom_blacklist: Optional[List[str]] = None,
        min_confidence: float = 0.8,
    ):
        """
        åˆå§‹åŒ–ç»¼åˆåå¤„ç†å™¨
        
        Args:
            enable_hallucination_detection: å¯ç”¨å¹»è§‰æ£€æµ‹
            enable_repetition_cleaning: å¯ç”¨é‡å¤æ¸…ç†
            enable_japanese_processing: å¯ç”¨æ—¥è¯­å¤„ç†
            custom_blacklist: è‡ªå®šä¹‰å¹»è§‰é»‘åå•
            min_confidence: å¹»è§‰æ£€æµ‹æœ€å°ç½®ä¿¡åº¦
        """
        self.enable_hallucination = enable_hallucination_detection
        self.enable_repetition = enable_repetition_cleaning
        self.enable_japanese = enable_japanese_processing
        
        # åˆå§‹åŒ–å­å¤„ç†å™¨
        if self.enable_hallucination:
            self.hallucination_detector = HallucinationDetector(
                custom_blacklist=custom_blacklist,
                min_confidence=min_confidence,
            )
        else:
            self.hallucination_detector = None
        
        if self.enable_repetition:
            self.repetition_cleaner = RepetitionCleaner()
        else:
            self.repetition_cleaner = None
        
        if self.enable_japanese:
            self.japanese_processor = JapanesePostProcessor()
        else:
            self.japanese_processor = None
        
        self.stats = PostProcessingStats()
        
        logger.info(f"ASRPostProcessor åˆå§‹åŒ–: hallucination={self.enable_hallucination}, "
                    f"repetition={self.enable_repetition}, japanese={self.enable_japanese}")
    
    def process_text(self, text: str) -> PostProcessingResult:
        """
        å¤„ç†å•ä¸ªæ–‡æœ¬
        
        Args:
            text: å¾…å¤„ç†æ–‡æœ¬
            
        Returns:
            PostProcessingResult
        """
        self.stats.total_segments += 1
        
        if not text or not text.strip():
            self.stats.empty_removed += 1
            return PostProcessingResult(
                original_text=text,
                processed_text='',
                is_hallucination=True,
                modifications=[{'type': 'empty_text'}],
            )
        
        modifications = []
        current_text = text
        is_hallucination = False
        
        # 1. å¹»è§‰æ£€æµ‹
        if self.hallucination_detector:
            is_hall, hall_info = self.hallucination_detector.detect(current_text)
            
            if is_hall:
                # äºŒæ¬¡éªŒè¯ï¼šç¡®ä¿ä¸æ˜¯æœ‰æ•ˆæ—¥è¯­å†…å®¹
                if self.japanese_processor and self.hallucination_detector.is_valid_japanese_content(current_text):
                    logger.debug(f"å¹»è§‰æ£€æµ‹è·³è¿‡ï¼ˆæœ‰æ•ˆæ—¥è¯­å†…å®¹ï¼‰: {current_text[:30]}...")
                else:
                    is_hallucination = True
                    self.stats.hallucinations_removed += 1
                    modifications.append(hall_info)
                    return PostProcessingResult(
                        original_text=text,
                        processed_text='',
                        is_hallucination=True,
                        modifications=modifications,
                    )
        
        # 2. é‡å¤æ¸…ç†
        if self.repetition_cleaner:
            # å…ˆæ£€æµ‹åŸå§‹æ–‡æœ¬æ˜¯å¦å‡ ä¹å…¨æ˜¯é‡å¤ï¼ˆå¦‚å¤§é‡ "ã†ã†ã†ã†"ï¼‰
            if self.repetition_cleaner.is_all_repetition(current_text):
                self.stats.hallucinations_removed += 1
                modifications.append({
                    'type': 'all_repetition',
                    'original': current_text[:50] + '...' if len(current_text) > 50 else current_text,
                    'category': 'repetition_hallucination',
                })
                return PostProcessingResult(
                    original_text=text,
                    processed_text='',
                    is_hallucination=True,
                    modifications=modifications,
                )
            
            cleaned, rep_mods = self.repetition_cleaner.clean(current_text)
            if rep_mods:
                modifications.extend(rep_mods)
                current_text = cleaned
                self.stats.repetitions_cleaned += 1
            
            # æ¸…ç†åå¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼ˆåŸæœ¬å¾ˆé•¿ä½†æ¸…ç†åå‡ ä¹æ²¡äº†ï¼‰ï¼Œä¹Ÿè§†ä¸ºå¹»è§‰
            if len(text) > 20 and len(current_text.strip()) < 5:
                self.stats.hallucinations_removed += 1
                modifications.append({
                    'type': 'cleaned_to_empty',
                    'original_len': len(text),
                    'cleaned_len': len(current_text),
                })
                return PostProcessingResult(
                    original_text=text,
                    processed_text='',
                    is_hallucination=True,
                    modifications=modifications,
                )
        
        # 3. æ—¥è¯­å¤„ç†
        if self.japanese_processor:
            processed, jp_mods = self.japanese_processor.process(current_text)
            if jp_mods:
                modifications.extend(jp_mods)
                current_text = processed
        
        return PostProcessingResult(
            original_text=text,
            processed_text=current_text,
            is_hallucination=False,
            modifications=modifications,
        )
    
    def process_segments(self, segments: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], PostProcessingStats]:
        """
        æ‰¹é‡å¤„ç†å­—å¹•æ®µ
        
        Args:
            segments: å­—å¹•æ®µåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« 'text' å­—æ®µ
            
        Returns:
            (processed_segments, stats)
        """
        processed = []
        
        for seg in segments:
            text = seg.get('text', '')
            result = self.process_text(text)
            
            if not result.is_hallucination and result.processed_text:
                new_seg = seg.copy()
                new_seg['text'] = result.processed_text
                if result.was_modified:
                    new_seg['_original_text'] = result.original_text
                    new_seg['_modifications'] = result.modifications
                processed.append(new_seg)
        
        logger.info(f"åå¤„ç†å®Œæˆ: è¾“å…¥ {len(segments)} æ®µ, è¾“å‡º {len(processed)} æ®µ, "
                    f"ç§»é™¤å¹»è§‰ {self.stats.hallucinations_removed}, "
                    f"æ¸…ç†é‡å¤ {self.stats.repetitions_cleaned}")
        
        return processed, self.stats
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.to_dict()
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.stats = PostProcessingStats()


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def postprocess_asr_text(
    text: str,
    enable_hallucination: bool = True,
    enable_repetition: bool = True,
    enable_japanese: bool = True,
) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šåå¤„ç† ASR æ–‡æœ¬
    
    Args:
        text: å¾…å¤„ç†æ–‡æœ¬
        enable_hallucination: å¯ç”¨å¹»è§‰æ£€æµ‹
        enable_repetition: å¯ç”¨é‡å¤æ¸…ç†
        enable_japanese: å¯ç”¨æ—¥è¯­å¤„ç†
        
    Returns:
        å¤„ç†åçš„æ–‡æœ¬ï¼ˆå¦‚æœæ˜¯å¹»è§‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ï¼‰
    """
    processor = ASRPostProcessor(
        enable_hallucination_detection=enable_hallucination,
        enable_repetition_cleaning=enable_repetition,
        enable_japanese_processing=enable_japanese,
    )
    result = processor.process_text(text)
    return result.processed_text


def is_hallucination(text: str) -> bool:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºå¹»è§‰
    
    Args:
        text: å¾…æ£€æµ‹æ–‡æœ¬
        
    Returns:
        æ˜¯å¦ä¸ºå¹»è§‰
    """
    detector = HallucinationDetector()
    is_hall, _ = detector.detect(text)
    return is_hall
